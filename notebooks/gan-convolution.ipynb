{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bde36b-ae69-4944-9369-4ab0c5a543e2",
   "metadata": {},
   "source": [
    "- https://medium.com/@YasinShafiei/deep-convolution-gan-on-fashion-mnist-using-pytorch-e99619940997\n",
    "- https://github.com/YasinShafiei/FashionMnist_DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca73b91-7195-4935-aeec-95a66986335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562c33bc-8245-40e5-a73b-c0baf94eb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d53ee-df37-4541-ae31-a63c561a2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb517e47-972d-4dc5-8c7b-43440244733d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a9834205130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7575de71-e6de-4e0c-936b-31f656e59699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ea566e-791c-4712-9b00-8030cd7448b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a67c9d6-509f-4221-b7d1-d470ccecac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347dccec-3a01-4e40-97c9-e122e0239f26",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c9db3a-993b-4ab9-9521-bf66dde20b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, noise_channels, image_channels, features):\n",
    "        super(Generator, self).__init__()\n",
    "        \"\"\"\n",
    "        In this function the generator model will be defined with all of it layers.\n",
    "        The generator model uses 4 ConvTranspose blocks. Each block containes \n",
    "        a ConvTranspose2d, BatchNorm2d and ReLU activation.\n",
    "        \"\"\"\n",
    "        # define the model\n",
    "        self.model = nn.Sequential(\n",
    "            # Transpose block 1\n",
    "            nn.ConvTranspose2d(noise_channels, features*16, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Transpose block 2\n",
    "            nn.ConvTranspose2d(features*16, features*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Transpose block 3\n",
    "            nn.ConvTranspose2d(features*8, features*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Transpose block 4\n",
    "            nn.ConvTranspose2d(features*4, features*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Last transpose block (different)\n",
    "            nn.ConvTranspose2d(features*2, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08012800-60c8-4587-bd77-b9a29a934157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \"\"\"\n",
    "        This function will define the Discriminator model with all the layers needed.\n",
    "        The model has 5 Conv blocks. The blocks have Conv2d, BatchNorm and LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        # define the model\n",
    "        self.model = nn.Sequential(\n",
    "            # define the first Conv block\n",
    "            nn.Conv2d(image_channels, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Conv block 2 \n",
    "            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Conv block 3\n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Conv block 4\n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Conv block 5 (different)\n",
    "            nn.Conv2d(features*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8082f3-0221-4fa1-83c9-8164d9d8abaa",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbaf6f21-5421-4e94-ac3d-52e1ec7f71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_SIZE = 64\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df88acbe-598e-4a2f-9d3e-78822a164c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 1\n",
    "noise_channels = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613ed05a-f92e-4e95-80d2-294ffa839adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_features = 64\n",
    "discriminator_features = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1511ae-f096-40b5-977f-699d5e17156c",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7d0117-5e85-4ea2-a78a-427264f572b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c52d9ff-5193-442b-886c-8a3641bace22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST(root=\"data\", train=True, transform=data_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2078d3ce-f297-480c-89b4-f8f74619723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003d8f8-dd95-4e94-8c50-f1773dadf128",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbeede0b-f3bd-4bd4-bfaa-70db01f44133",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(noise_channels, image_channels, generator_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a4a403-77df-48a9-a1e0-aef0561a6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(image_channels, discriminator_features).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4804109-921c-40f9-9bd4-7b2f1c55f9f0",
   "metadata": {},
   "source": [
    "## optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d9ba71-4ef8-4704-98af-7f8f5039e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c542eaf7-acc1-4cac-ba30-c8ff9bfd9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6abdd-c9e8-4a2c-977f-44267b198992",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35fa8420-dae4-4911-b6bf-3b3d45832164",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd8b82-5973-4b9c-8c15-1edde54e1aba",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e030aaef-3ede-43f5-976f-6c73c49cd212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): ConvTranspose2d(256, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (12): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de14439b-f07c-45ca-bb09-9544b57ebae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac98d30d-b75f-4fa1-8a24-9a01c859e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels for fake images and real images for the discriminator\n",
    "fake_label = 0\n",
    "real_label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd20ccdd-1402-4e3e-bf2e-761ab5aadfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fixed noise \n",
    "fixed_noise = torch.randn(64, noise_channels, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b083f67b-557f-4f58-bcc3-95e6264bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the writers for tensorboard\n",
    "shutil.rmtree('tensorboard/fashion')\n",
    "tb_writer = SummaryWriter('tensorboard/fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad16d772-98c8-43f5-8716-ebec95f4b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a step\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530fcb51-02f5-4be1-9d7f-fc88d6684a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 000, step: 000001, D: 1.3850, G: 21.0365\n",
      "epoch: 000, step: 000002, D: 3.1223, G: 3.0181\n",
      "epoch: 000, step: 000003, D: 0.6621, G: 2.4211\n",
      "epoch: 000, step: 000004, D: 1.5348, G: 7.0533\n",
      "epoch: 000, step: 000005, D: 0.7098, G: 2.7755\n",
      "epoch: 001, step: 000006, D: 2.3670, G: 6.1634\n",
      "epoch: 001, step: 000007, D: 1.3009, G: 1.3653\n",
      "epoch: 001, step: 000008, D: 1.3215, G: 1.1633\n",
      "epoch: 001, step: 000009, D: 1.4442, G: 1.7016\n",
      "epoch: 001, step: 000010, D: 1.4265, G: 0.9652\n",
      "epoch: 002, step: 000011, D: 1.4524, G: 0.7211\n",
      "epoch: 002, step: 000012, D: 1.2780, G: 1.2259\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # get the batch size \n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        # real data\n",
    "        real_samples = data.to(device)\n",
    "        real_label = (torch.ones(batch_size) * 0.9).to(device)\n",
    "\n",
    "        # initialize the discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # train the discriminator model on real data\n",
    "        real_output = discriminator(real_samples).reshape(-1)\n",
    "        real_discriminator_loss = loss(real_output, real_label)\n",
    "\n",
    "        # fake data\n",
    "        fake_vectors = torch.randn(batch_size, noise_channels, 1, 1).to(device)\n",
    "        fake_labels = (torch.ones(batch_size) * 0.1).to(device)\n",
    "        fake_samples = generator(fake_vectors)\n",
    "\n",
    "        # train the discriminator model on fake (generated) data\n",
    "        fake_output = discriminator(fake_samples.detach()).reshape(-1)\n",
    "        fake_discriminator_loss = loss(fake_output, fake_labels)\n",
    "\n",
    "        # calculate the final discriminator loss\n",
    "        discriminator_loss = real_discriminator_loss + fake_discriminator_loss\n",
    "\n",
    "        # apply the optimizer and gradient to the discriminator\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # initialize the generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # generated data\n",
    "        generated_samples = fake_samples\n",
    "        generated_labels = (torch.ones(batch_size) * 1.0).to(device)\n",
    "\n",
    "        # train the generator model\n",
    "        generated_output = discriminator(generated_samples).reshape(-1)\n",
    "        generator_loss = loss(generated_output, generated_labels)\n",
    "        \n",
    "        # apply the optimizer and gradient to the generator\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # print losses in console and tensorboard\n",
    "        if batch_idx % 50 == 0:\n",
    "            step += 1\n",
    "\n",
    "            # print everything\n",
    "            print(f\"epoch: {epoch:03d}, step: {step:06d}, D: {discriminator_loss:.4f}, G: {generator_loss:.4f}\")\n",
    "\n",
    "            # test the model\n",
    "            with torch.no_grad():\n",
    "                # generate fake images\n",
    "                fake_images = generator(fixed_noise)\n",
    "\n",
    "            # report fake images \n",
    "            img_grid_fake = torchvision.utils.make_grid(fake_images[:40], normalize=True)\n",
    "            tb_writer.add_image(\"generated\", img_grid_fake, global_step=step)\n",
    "\n",
    "            # report real images \n",
    "            img_grid_real = torchvision.utils.make_grid(data[:40], normalize=True)\n",
    "            tb_writer.add_image(\"real\", img_grid_real, global_step=step)\n",
    "\n",
    "            # report losses\n",
    "            loss_dict = {'generator': generator_loss, 'discriminator': discriminator_loss} \n",
    "            tb_writer.add_scalars('loss', loss_dict, global_step=step)\n",
    "\n",
    "            # flush tensorboard\n",
    "            tb_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647c9f3-a168-4c81-9a69-1446a54c287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfe983-227b-418b-ac97-2877f82f0106",
   "metadata": {},
   "source": [
    "## result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2d5a5-71cc-4b13-90f9-625de535a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # generate fake images\n",
    "    fake_images = generator(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42199fb-284d-46a0-8587-bf07bceebc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report fake images \n",
    "img_grid_fake = torchvision.utils.make_grid(fake_images[:40], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519924ec-afc4-40d7-9f0c-cac6a3889498",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_grid_fake.cpu().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e94699-3927-4d57-a18d-1ff50b21e540",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a8b4d-47c2-4469-ac26-a323d83e66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6f483-60c9-466b-aa75-9de178e161cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboard/fashion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
